{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eacd34b4",
   "metadata": {},
   "source": [
    "## A8: Ensemble Learning for Complex Regression Modeling on Bike Share Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a29bf21",
   "metadata": {},
   "source": [
    "### Part A: Data Preprocessing and Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92633f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
       "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
       "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
       "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
       "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
       "\n",
       "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
       "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
       "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
       "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
       "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
       "4           1  0.24  0.2879  0.75        0.0       0           1    1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "df= pd.read_csv(\"hour.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94b3710f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17379, 17)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87e1fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Target = df['cnt']\n",
    "Features = df.drop(columns=['cnt', 'dteday', 'instant', 'casual', 'registered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8adeb3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17379, 17)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f65fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17379 entries, 0 to 17378\n",
      "Data columns (total 17 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   instant     17379 non-null  int64  \n",
      " 1   dteday      17379 non-null  object \n",
      " 2   season      17379 non-null  int64  \n",
      " 3   yr          17379 non-null  int64  \n",
      " 4   mnth        17379 non-null  int64  \n",
      " 5   hr          17379 non-null  int64  \n",
      " 6   holiday     17379 non-null  int64  \n",
      " 7   weekday     17379 non-null  int64  \n",
      " 8   workingday  17379 non-null  int64  \n",
      " 9   weathersit  17379 non-null  int64  \n",
      " 10  temp        17379 non-null  float64\n",
      " 11  atemp       17379 non-null  float64\n",
      " 12  hum         17379 non-null  float64\n",
      " 13  windspeed   17379 non-null  float64\n",
      " 14  casual      17379 non-null  int64  \n",
      " 15  registered  17379 non-null  int64  \n",
      " 16  cnt         17379 non-null  int64  \n",
      "dtypes: float64(4), int64(12), object(1)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295dd053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping columns: (17379, 13)\n",
      "After one-hot encoding: (17379, 54)\n",
      "Train set shape: (13903, 53)\n",
      "Test set shape: (3476, 53)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df = df.drop(columns=[\"instant\", \"dteday\", \"casual\", \"registered\"])\n",
    "print(\"After dropping columns:\", df.shape)\n",
    "\n",
    "categorical_features = [\"season\",\"yr\", \"weathersit\", \"mnth\", \"hr\", \"weekday\"]\n",
    "\n",
    "df = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n",
    "print(\"After one-hot encoding:\", df.shape)\n",
    "\n",
    "\n",
    "X = df.drop(columns=[\"cnt\"])\n",
    "y = df[\"cnt\"]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41cdb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree RMSE: 118.456\n",
      "Linear Regression RMSE: 100.446\n",
      "Baseline Model: Linear Regression (RMSE = 100.446)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "dt_model = DecisionTreeRegressor(max_depth=6, random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "rmse_dt = np.sqrt(mean_squared_error(y_test, y_pred_dt))\n",
    "print(f\"Decision Tree RMSE: {rmse_dt:.3f}\")\n",
    "\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "print(f\"Linear Regression RMSE: {rmse_lr:.3f}\")\n",
    "\n",
    "#Compare and Choose Baseline\n",
    "if rmse_dt < rmse_lr:\n",
    "    print(f\"Baseline Model: Decision Tree (RMSE = {rmse_dt:.3f})\")\n",
    "else:\n",
    "    print(f\"Baseline Model: Linear Regression (RMSE = {rmse_lr:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd980b",
   "metadata": {},
   "source": [
    "### Part B: Ensemble Techniques for Bias and Variance Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0516a8",
   "metadata": {},
   "source": [
    "#### 1. Bagging (Variance Reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f80e031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Decision Tree RMSE: 118.4555\n",
      "Bagging Regressor (50 trees) RMSE: 112.3281\n",
      "RMSE improvement vs single tree: 5.17%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "single_dt = DecisionTreeRegressor(max_depth=6, random_state=42)\n",
    "single_dt.fit(X_train, y_train)\n",
    "y_pred_dt = single_dt.predict(X_test)\n",
    "rmse_dt = np.sqrt(mean_squared_error(y_test, y_pred_dt))\n",
    "print(f\"Single Decision Tree RMSE: {rmse_dt:.4f}\")\n",
    "\n",
    "\n",
    "bag_model = BaggingRegressor(\n",
    "    estimator=DecisionTreeRegressor(max_depth=6),\n",
    "    n_estimators=50,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "bag_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_bag = bag_model.predict(X_test)\n",
    "rmse_bag = np.sqrt(mean_squared_error(y_test, y_pred_bag))\n",
    "print(f\"Bagging Regressor (50 trees) RMSE: {rmse_bag:.4f}\")\n",
    "\n",
    "\n",
    "improvement = (rmse_dt - rmse_bag) / rmse_dt * 100.0\n",
    "print(f\"RMSE improvement vs single tree: {improvement:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a345743b",
   "metadata": {},
   "source": [
    "The single Decision Tree had an RMSE of 118.46, while the Bagging Regressor (50 trees) achieved an RMSE of 112.33, showing about a 5.17% improvement in predictive performance.\n",
    "\n",
    "This reduction in RMSE indicates that bagging effectively reduced variance compared to the single tree baseline. Decision Trees are known to be high-variance models — small changes in training data can lead to big differences in the model structure and predictions. Bagging combats this by training multiple trees on different random subsets of the data and averaging their outputs.\n",
    "\n",
    "Averaging across 50 trees stabilizes the predictions, smooths out noise, and makes the overall model less sensitive to fluctuations in the training data. The moderate improvement is expected here since the base trees were already somewhat regularized with max_depth=6, which limits overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac222be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c947bc69",
   "metadata": {},
   "source": [
    "#### 2. Boosting (Bias Reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335e35fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Regressor RMSE: 78.9652\n",
      "Single Decision Tree RMSE: 118.4555\n",
      "Bagging Regressor RMSE: 112.3281\n",
      "Boosting achieved the best result — supports the bias reduction hypothesis.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=100,     \n",
    "    learning_rate=0.1,    \n",
    "    max_depth=3,          \n",
    "    random_state=42\n",
    ")\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "rmse_gb = np.sqrt(mean_squared_error(y_test, y_pred_gb))\n",
    "print(f\"Gradient Boosting Regressor RMSE: {rmse_gb:.4f}\")\n",
    "\n",
    "print(f\"Single Decision Tree RMSE: {rmse_dt:.4f}\")\n",
    "print(f\"Bagging Regressor RMSE: {rmse_bag:.4f}\")\n",
    "\n",
    "\n",
    "if rmse_gb < min(rmse_dt, rmse_bag):\n",
    "    print(\"Boosting achieved the best result — supports the bias reduction hypothesis.\")\n",
    "else:\n",
    "    print(\"Boosting did not outperform both models — check tuning or data bias.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b9e42f",
   "metadata": {},
   "source": [
    "Unlike bagging, which builds multiple independent trees in parallel to reduce variance, boosting trains trees sequentially, with each new tree focusing on correcting the previous model’s errors. This step-by-step correction process allows the ensemble to capture more complex patterns and relationships that a single or bagged tree might miss — hence reducing systematic bias.\n",
    "\n",
    "The large RMSE drop (≈ 33% improvement over bagging) supports the hypothesis that boosting targets and minimizes bias, leading to more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41ca46e",
   "metadata": {},
   "source": [
    "### Part C: Stacking for Optimal Performance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46b10e3",
   "metadata": {},
   "source": [
    "Stacking is an ensemble learning technique that combines multiple diverse models (called base learners) to improve predictive performance.\n",
    "\n",
    "Level-0 (Base Learners):\n",
    "Multiple models are trained on the same training data, but each learns different aspects of the pattern ( KNN is instance-based, Tree-based models capture nonlinear splits, etc.).\n",
    "\n",
    "Level-1 (Meta-Learner):\n",
    "The predictions of these base models (on unseen or validation data) are used as new input features to train a meta-model (Ridge Regression).\n",
    "\n",
    "The meta-learner learns how to optimally weight and combine the outputs of the base learners. For example, giving more weight to the model that performs better in certain regions of the data.\n",
    "\n",
    "Bagging reduces variance and Boosting reduces bias\n",
    "Stacking combines the strengths of different learners to improve both bias and variance trade-off.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9267b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Regressor RMSE: 67.0248\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "#Base Learners (Level-0)\n",
    "base_learners = [\n",
    "    (\"knn\", KNeighborsRegressor(n_neighbors=5)),\n",
    "    (\"bagging\", BaggingRegressor(\n",
    "        estimator=DecisionTreeRegressor(max_depth=6),\n",
    "        n_estimators=50,\n",
    "        random_state=42\n",
    "    )),\n",
    "    (\"gboost\", GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        random_state=42\n",
    "    ))\n",
    "]\n",
    "\n",
    "# Meta-Learner (Level-1)\n",
    "meta_learner = Ridge(alpha=1.0)\n",
    "\n",
    "#Stacking Regressor\n",
    "stack_model = StackingRegressor(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_learner,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#Train Stacking Regressor\n",
    "stack_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_stack = stack_model.predict(X_test)\n",
    "rmse_stack = np.sqrt(mean_squared_error(y_test, y_pred_stack))\n",
    "print(f\"Stacking Regressor RMSE: {rmse_stack:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ebb7c",
   "metadata": {},
   "source": [
    "### Part D: Final Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b80e62ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RMSE Comparison Table:\n",
      "\n",
      "                                          Model      RMSE\n",
      "0                            Stacking Regressor   67.0248\n",
      "1                   Gradient Boosting Regressor   78.9652\n",
      "2                             Bagging Regressor  112.3281\n",
      "3  Baseline (Linear Regression / Decision Tree)  118.4555\n",
      "\n",
      " Best Performing Model:\n",
      "Stacking Regressor (RMSE = 67.0248)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rmse_results = {\n",
    "    \"Model\": [\n",
    "        \"Baseline (Linear Regression / Decision Tree)\",\n",
    "        \"Bagging Regressor\",\n",
    "        \"Gradient Boosting Regressor\",\n",
    "        \"Stacking Regressor\"\n",
    "    ],\n",
    "    \"RMSE\": [\n",
    "        118.4555,  \n",
    "        112.3281,  \n",
    "        78.9652,   \n",
    "        67.0248 \n",
    "    ]\n",
    "}\n",
    "\n",
    "rmse_df = pd.DataFrame(rmse_results)\n",
    "rmse_df = rmse_df.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
    "\n",
    "print(\" RMSE Comparison Table:\\n\")\n",
    "print(rmse_df)\n",
    "\n",
    "# best model\n",
    "best_model = rmse_df.iloc[0]\n",
    "print(\"\\n Best Performing Model:\")\n",
    "print(f\"{best_model['Model']} (RMSE = {best_model['RMSE']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88582a5b",
   "metadata": {},
   "source": [
    "This performance highlights how stacking effectively combines the strengths of multiple diverse learners. While the single Decision Tree suffered from high variance and limited generalization, and Bagging mainly reduced variance through averaging, Stacking goes a step further. It integrates multiple base models such as KNN, Bagging, and Gradient Boosting, each capturing different aspects of the data’s structure.\n",
    "\n",
    "The meta-learner (Ridge Regression) then learns to optimally weight their predictions, leveraging the complementary strengths of these models.\n",
    "This results in a balanced ensemble that reduces both bias and variance, leading to superior generalization on unseen data.\n",
    "\n",
    "Stacking Regressor outperformed the single model baseline because:\n",
    "\n",
    "1. It reduced bias by including complex learners like Gradient Boosting.\n",
    "\n",
    "2. It reduced variance by averaging across multiple diverse models.\n",
    "\n",
    "3. It exploited model diversity, ensuring the weaknesses of one model are compensated by the strengths of others.\n",
    "\n",
    "Hence, Stacking demonstrates the ideal trade-off in the bias–variance spectrum, achieving the most accurate and stable predictions overall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
